# -*- coding: utf-8 -*-
"""AI Ethics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iypAjZIlkaX3c8RG9l92erUtSmYsnLQB
"""



import pandas as pd

# Load the dataset
file_path = 'KaggleV2-May-2016.csv'
data = pd.read_csv(file_path)

# Display the first few rows to inspect the dataset structure
data.head()

data.info()

# # Convert ScheduledDay and AppointmentDay to datetime
# data['ScheduledDay'] = pd.to_datetime(data['ScheduledDay'], errors='coerce')
# # data['AppointmentDay'] = pd.to_datetime(data['AppointmentDay'], errors='coerce')
# data['AppointmentDay'] = pd.to_datetime(data['AppointmentDay']).apply(lambda x: x.date())

# Convert datetime columns to timezone-naive
data['ScheduledDay'] = pd.to_datetime(data['ScheduledDay']).dt.tz_localize(None)
data['AppointmentDay'] = pd.to_datetime(data['AppointmentDay']).dt.tz_localize(None)

# Create new features from datetime columns
data['ScheduledDayOfWeek'] = data['ScheduledDay'].dt.dayofweek
data['AppointmentDayOfWeek'] = data['AppointmentDay'].dt.dayofweek
data['DaysBetween'] = (data['AppointmentDay'] - data['ScheduledDay']).dt.days

# Drop original datetime columns (if not needed further)
data.drop(columns=['ScheduledDay', 'AppointmentDay'], inplace=True)

# Encode categorical variables
data['Gender'] = data['Gender'].map({'F': 0, 'M': 1})

# Proceed with the rest of the preprocessing

# Check for null values
null_counts = data.isnull().sum()

# Convert No-show to binary (Yes=1, No=0)
data['No-show'] = data['No-show'].map({'Yes': 1, 'No': 0})

# # Encode Gender as binary (F=0, M=1)
# data['Gender'] = data['Gender'].map({0: 'F', 1: "M"})

# Summary of preprocessing steps applied
{
    "Null values per column": null_counts.to_dict(),
    "Unique values in No-show": data['No-show'].unique(),
    "Unique values in Gender": data['Gender'].unique(),
}

print(data.columns)
print(data['Gender'].head())
print(data['Gender'].isnull().sum())

# Encode Neighbourhood using one-hot encoding
neighbourhood_encoded = pd.get_dummies(data['Neighbourhood'], prefix='Neighbourhood')

# Combine the encoded neighbourhood with the main dataset and drop the original column
data = pd.concat([data, neighbourhood_encoded], axis=1).drop(columns=['Neighbourhood'])

# Display the dataset structure after encoding
data.info()

import matplotlib.pyplot as plt
import seaborn as sns

# Set a general style for the plots
sns.set(style="whitegrid")

# Function to plot distributions
def plot_distribution(column, title):
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True, color="skyblue")
    plt.title(title, fontsize=14)
    plt.xlabel(column, fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.show()

# 1. Distribution of Age
plot_distribution("Age", "Age Distribution")

# 2. Gender distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Gender', palette='viridis')
plt.title("Gender Distribution", fontsize=14)
plt.xlabel("Gender (0: Female, 1: Male)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 3. Distribution of No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='No-show', palette='magma')
plt.title("No-show Distribution", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 4. Relationship between Age and No-show
plt.figure(figsize=(8, 5))
sns.boxplot(data=data, x='No-show', y='Age', palette='coolwarm')
plt.title("Age vs. No-show", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Age", fontsize=12)
plt.show()

# 5. Correlation heatmap for numerical columns
plt.figure(figsize=(12, 8))
numerical_data = data.select_dtypes(include=['int64', 'float64'])
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix", fontsize=16)
plt.show()

# 6. SMS_received vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='SMS_received', hue='No-show', palette='viridis')
plt.title("SMS Received vs. No-show", fontsize=14)
plt.xlabel("SMS Received (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 7. Scholarship vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Scholarship', hue='No-show', palette='muted')
plt.title("Scholarship vs. No-show", fontsize=14)
plt.xlabel("Scholarship (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 8. Additional statistics for Age grouped by No-show
age_stats = data.groupby('No-show')['Age'].describe()
age_stats





import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score
import numpy as np

# Set a general style for the plots
sns.set(style="whitegrid")

# Function to plot distributions
def plot_distribution(column, title):
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True, color="skyblue")
    plt.title(title, fontsize=14)
    plt.xlabel(column, fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.show()

# 1. Distribution of Age
plot_distribution("Age", "Age Distribution")

# 2. Gender distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Gender', palette='viridis')
plt.title("Gender Distribution", fontsize=14)
plt.xlabel("Gender (0: Female, 1: Male)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 3. Distribution of No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='No-show', palette='magma')
plt.title("No-show Distribution", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 4. Relationship between Age and No-show
plt.figure(figsize=(8, 5))
sns.boxplot(data=data, x='No-show', y='Age', palette='coolwarm')
plt.title("Age vs. No-show", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Age", fontsize=12)
plt.show()

# 5. Correlation heatmap for numerical columns
plt.figure(figsize=(12, 8))
numerical_data = data.select_dtypes(include=['int64', 'float64'])
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix", fontsize=16)
plt.show()

# 6. SMS_received vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='SMS_received', hue='No-show', palette='viridis')
plt.title("SMS Received vs. No-show", fontsize=14)
plt.xlabel("SMS Received (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 7. Scholarship vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Scholarship', hue='No-show', palette='muted')
plt.title("Scholarship vs. No-show", fontsize=14)
plt.xlabel("Scholarship (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 8. Additional statistics for Age grouped by No-show
age_stats = data.groupby('No-show')['Age'].describe()
print(age_stats)

# Preprocessing for Machine Learning
# Splitting the dataset into features and target
X = data.drop(columns=['No-show'])
y = data['No-show']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(np.array(X_train))
X_test = scaler.transform(np.array(X_test))

# Model Training - Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Evaluation Metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
print(f"Accuracy: {accuracy:.2f}")
print(f"ROC AUC Score: {roc_auc:.2f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.title("ROC Curve", fontsize=14)
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.legend()
plt.show()






import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score
import numpy as np
import shap
from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, false_negative_rate
from fairlearn.reductions import GridSearch, DemographicParity
from fairlearn.widget import FairlearnDashboard

# Set a general style for the plots
sns.set(style="whitegrid")

# Function to plot distributions
def plot_distribution(column, title):
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True, color="skyblue")
    plt.title(title, fontsize=14)
    plt.xlabel(column, fontsize=12)
    plt.ylabel("Frequency", fontsize=12)
    plt.show()

# 1. Distribution of Age
plot_distribution("Age", "Age Distribution")

# 2. Gender distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Gender', palette='viridis')
plt.title("Gender Distribution", fontsize=14)
plt.xlabel("Gender (0: Female, 1: Male)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 3. Distribution of No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='No-show', palette='magma')
plt.title("No-show Distribution", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.show()

# 4. Relationship between Age and No-show
plt.figure(figsize=(8, 5))
sns.boxplot(data=data, x='No-show', y='Age', palette='coolwarm')
plt.title("Age vs. No-show", fontsize=14)
plt.xlabel("No-show (0: Show, 1: No-show)", fontsize=12)
plt.ylabel("Age", fontsize=12)
plt.show()

# 5. Correlation heatmap for numerical columns
plt.figure(figsize=(12, 8))
numerical_data = data.select_dtypes(include=['int64', 'float64'])
correlation_matrix = numerical_data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix", fontsize=16)
plt.show()

# 6. SMS_received vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='SMS_received', hue='No-show', palette='viridis')
plt.title("SMS Received vs. No-show", fontsize=14)
plt.xlabel("SMS Received (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 7. Scholarship vs. No-show
plt.figure(figsize=(6, 4))
sns.countplot(data=data, x='Scholarship', hue='No-show', palette='muted')
plt.title("Scholarship vs. No-show", fontsize=14)
plt.xlabel("Scholarship (0: No, 1: Yes)", fontsize=12)
plt.ylabel("Count", fontsize=12)
plt.legend(title="No-show", labels=["Show", "No-show"])
plt.show()

# 8. Additional statistics for Age grouped by No-show
age_stats = data.groupby('No-show')['Age'].describe()
print(age_stats)

# Preprocessing for Machine Learning
# Splitting the dataset into features and target
X = data.drop(columns=['No-show'])
y = data['No-show']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Training - Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Evaluation Metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)
print(f"Accuracy: {accuracy:.2f}")
print(f"ROC AUC Score: {roc_auc:.2f}")

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.title("ROC Curve", fontsize=14)
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
#plt.legend()
plt.show()

# SHAP values for interpretability
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary Plot
shap.summary_plot(shap_values[1], X_test, plot_type="bar")

# Fairness Metrics
metric_frame = MetricFrame(
    metrics={"accuracy": accuracy_score, "selection_rate": selection_rate},
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=data.loc[y_test.index, 'Gender']
)
print("Fairness Metrics:")
print(metric_frame.by_group)

# Fairlearn Dashboard
FairlearnDashboard(
    sensitive_features=data.loc[y_test.index, 'Gender'],
    y_true=y_test,
    y_pred=y_pred
)

def selection_rate(y_pred):
    return sum(y_pred) / len(y_pred)

def false_positive_rate(y_true, y_pred):
    return sum((y_pred == 1) & (y_true == 0)) / sum(y_true == 0)

def false_negative_rate(y_true, y_pred):
    return sum((y_pred == 0) & (y_true == 1)) / sum(y_true == 1)

from sklearn.metrics import accuracy_score

# Example of manual fairness metric calculation
def fairness_metrics(y_true, y_pred, sensitive_features):
    groups = np.unique(sensitive_features)
    metrics = {}
    for group in groups:
        group_indices = sensitive_features == group
        metrics[group] = accuracy_score(y_true[group_indices], y_pred[group_indices])
    return metrics

fairness_results = fairness_metrics(
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=data.loc[y_test.index, 'Gender']
)
print(f"Fairness Metrics by Group: {fairness_results}")

# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# import tensorflow as tf
# import pandas as pd
# import numpy as np



# # Read the Data

# data = pd.read_csv('KaggleV2-May-2016.csv')
# data['ScheduledDay'] = pd.to_datetime(data['ScheduledDay'])
# data['AppointmentDay'] = pd.to_datetime(data['AppointmentDay'])

# # Drop unnecessary columns
# processed_data = data.drop(['PatientId', 'AppointmentID'], axis=1)

# # Calculate Time Difference between appointment dates
# processed_data['TimeDifference'] = (data['AppointmentDay'] - data['ScheduledDay']).dt.days

# # Drop date-time columns after extracting necessary information
# processed_data = processed_data.drop(['ScheduledDay', 'AppointmentDay'], axis=1)

# # Map 'Yes' and 'No' to 1 and 0 respectively
# processed_data['No-show'] = processed_data['No-show'].map({'Yes': 1, 'No': 0})

# # Encode categorical columnsfrom sklearn.model_selection import train_test_split

# from sklearn.preprocessing import StandardScaler
# import tensorflow as tf
# import pandas as pd
# import numpy as np

# # Read the Data
# data = pd.read_csv('KaggleV2-May-2016.csv')

# # Perform your initial processing steps here, e.g., calculating TimeDifference, etc.

# # Create dummy variables BEFORE dropping or modifying the original columns
# processed_data = pd.get_dummies(processed_data, columns=['Gender', 'Neighbourhood'], prefix=['Gender', 'Neighbourhood'])

# # Select features and target for modeling
# X = processed_data.drop('No-show', axis=1)
# y = processed_data['No-show']

# # Split data: 80:20
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Feature scaling
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# # Initialize the neural network model and use ADAM optimizer
# MDL = tf.keras.models.Sequential([
#     tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
#     tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.BatchNormalization(),
#     tf.keras.layers.Dense(1, activation='sigmoid')
# ])

# # Compile the model
# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
#     initial_learning_rate=1e-3,
#     decay_steps=10000,
#     decay_rate=0.9
# )
# MDL.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=['accuracy'])

# # Train the model
# MDL.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# # Evaluate the model
# loss, accuracy = MDL.evaluate(X_test, y_test)
# print("LOSS Value: ", loss)
# print("Acc Score: ", accuracy)



# # Necessary Modules
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# import tensorflow as tf
# import pandas as pd
# import numpy as np
# from imblearn.over_sampling import SMOTE
# import matplotlib.pyplot as plt
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score, precision_score, recall_score, precision_recall_curve
# import shap
# from sklearn.utils import class_weight  # Import for calculating class weights


# if tf.config.list_physical_devices('GPU'):
#     print("GPU is available and being used by TensorFlow.")
# else:
#     print("GPU not available. Check runtime settings.")

# # Read the Data
# data = pd.read_csv('KaggleV2-May-2016.csv')
# data['ScheduledDay'] = pd.to_datetime(data['ScheduledDay'])

# # Drop unnecessary columns
# processed_data = data.drop(['PatientId', 'AppointmentID'], axis=1)

# # Calculate Time Difference between appointment dates
# processed_data['TimeDifference'] = (data['AppointmentDay'] - data['ScheduledDay']).dt.days

# # Drop date-time columns after extracting necessary information
# processed_data = processed_data.drop(['ScheduledDay', 'AppointmentDay'], axis=1)

# # Map 'Yes' and 'No' to 1 and 0 respectively
# processed_data['No-show'] = processed_data['No-show'].map({'Yes': 1, 'No': 0})

# # Apply pseudonymization to Neighbourhood (replace original values with encoded values)
# from sklearn.preprocessing import LabelEncoder
# label_encoder = LabelEncoder()
# processed_data['Neighbourhood_encoded'] = label_encoder.fit_transform(processed_data['Neighbourhood'])

# # Drop the original Neighbourhood column as it's now pseudonymized
# processed_data = processed_data.drop('Neighbourhood', axis=1)

# # Encode categorical columns
# processed_data = pd.get_dummies(processed_data, columns=['Gender'], prefix=['Gender'])

# # Select features and target for modeling
# X = processed_data.drop('No-show', axis=1)
# y = processed_data['No-show']

# # Analyze representation of demographic features
# gender_counts = processed_data['Gender_M'].value_counts()
# neighborhood_counts = processed_data.filter(like='Neighbourhood').sum()
# processed_data['Age'].fillna(processed_data['Age'].median(), inplace=True)  # Impute missing values in Age
# age_groups = pd.cut(processed_data['Age'], bins=[0, 18, 40, 60, 100], labels=['0-18', '19-40', '41-60', '61+']).value_counts()
# sms_counts = processed_data['SMS_received'].value_counts()
# print("Gender Distribution:\n", gender_counts)
# print("Neighborhood Distribution:\n", neighborhood_counts)
# print("Age Distribution:\n", age_groups)
# print("SMS Received Distribution:\n", sms_counts)

